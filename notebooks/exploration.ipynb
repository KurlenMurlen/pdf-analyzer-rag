{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R&D Project Auditor Exploration\n",
    "\n",
    "This notebook is used for exploratory data analysis and experimentation with the R&D Project Auditor application. It allows for interactive development and testing of various components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: /Users/tarsobertolini/Documents/PUR-PRO/pur-auditor-rag\n",
      "✅ Libraries imported. Using LLM Model: amazon.titan-text-express-v1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Robustly set up the project path\n",
    "current_path = Path(os.getcwd())\n",
    "if current_path.name == 'notebooks':\n",
    "    project_root = current_path.parent\n",
    "else:\n",
    "    project_root = current_path\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root set to: {project_root}\")\n",
    "\n",
    "# Force reload environment variables (override existing)\n",
    "load_dotenv(project_root / \".env\", override=True)\n",
    "\n",
    "try:\n",
    "    # Import modules\n",
    "    import src.ingest\n",
    "    import src.rag_engine\n",
    "    import src.auditor\n",
    "    import src.config\n",
    "    \n",
    "    # Force reload to pick up file changes\n",
    "    importlib.reload(src.config)\n",
    "    importlib.reload(src.ingest)\n",
    "    importlib.reload(src.rag_engine)\n",
    "    importlib.reload(src.auditor)\n",
    "    \n",
    "    # Import classes from reloaded modules\n",
    "    from src.ingest import IngestionEngine\n",
    "    from src.rag_engine import RAGEngine\n",
    "    from src.auditor import AuditorAgent\n",
    "    from src.config import settings\n",
    "    from langchain_community.chat_models import ChatOpenAI\n",
    "    \n",
    "    print(f\"✅ Libraries imported. Using LLM Model: {settings.LLM_MODEL_ID}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import Error: {e}\")\n",
    "    print(\"Please ensure you are running this notebook from the correct environment and have installed dependencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ../data/Draft-PUR-Tv3.0-Principal.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.ingest:Successfully loaded 37 pages from ../data/Draft-PUR-Tv3.0-Principal.pdf\n",
      "INFO:src.ingest:Split 37 pages into 75 chunks.\n",
      "INFO:src.ingest:Split 37 pages into 75 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 75 chunks.\n",
      "Sample chunk content:\n",
      "PLANO DE UTILIZAÇÃO DE RECURSOS – PUR\n",
      "N° xxx/2025\n",
      "PROGRAMA: TV 3.0\n",
      "PERÍODO: JANEIRO/2025 A JUNHO/2027\n",
      "MANAUS/AM\n",
      "AGOSTO/2025...\n"
     ]
    }
   ],
   "source": [
    "# 1. Ingestion Phase\n",
    "\n",
    "# Ensure imports are available if this cell is run out of order\n",
    "if 'IngestionEngine' not in globals():\n",
    "    print(\"⚠️ IngestionEngine not found. Attempting to import...\")\n",
    "    try:\n",
    "        from src.ingest import IngestionEngine\n",
    "    except ImportError:\n",
    "        print(\"❌ Could not import IngestionEngine. Please run the 'Import necessary libraries' cell above.\")\n",
    "\n",
    "# Initialize the ingestion engine\n",
    "if 'IngestionEngine' in globals():\n",
    "    ingestor = IngestionEngine(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "    # Define the path to your uploaded PDF\n",
    "    pdf_path = \"../data/Draft-PUR-Tv3.0-Principal.pdf\" \n",
    "\n",
    "    # Check if file exists before running\n",
    "    if not os.path.exists(pdf_path):\n",
    "        # Try absolute path if relative fails\n",
    "        pdf_path = os.path.join(os.getcwd(), \"..\", \"data\", \"Draft-PUR-Tv3.0-Principal.pdf\")\n",
    "        \n",
    "    if os.path.exists(pdf_path):\n",
    "        print(f\"Found file: {pdf_path}\")\n",
    "        documents = ingestor.process_file(pdf_path)\n",
    "        print(f\"Processed {len(documents)} chunks.\")\n",
    "        \n",
    "        if documents:\n",
    "            print(f\"Sample chunk content:\\n{documents[0].page_content[:500]}...\")\n",
    "    else:\n",
    "        print(f\"❌ File not found at {pdf_path}. Please ensure 'Draft-PUR-Tv3.0-Principal.pdf' is in the 'data' folder.\")\n",
    "else:\n",
    "    print(\"❌ Cannot proceed without IngestionEngine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:src.rag_engine:Creating vector store with 75 chunks...\n",
      "INFO:src.rag_engine:Creating vector store with 75 chunks...\n",
      "INFO:src.rag_engine:Vector store saved to vector_store_faiss\n",
      "INFO:src.rag_engine:Loading vector store from vector_store_faiss...\n",
      "INFO:src.rag_engine:Vector store saved to vector_store_faiss\n",
      "INFO:src.rag_engine:Loading vector store from vector_store_faiss...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 docs.\n",
      "TRL (Technology Readiness Level), a situação presente pode ser enquadrada entre TRL 2\n",
      "e TRL 3, correspondente à formulação do conceito tecnológico e à comprovação\n",
      "experimental de princípios básicos em ambiente controlado, sem aplicação prática no\n",
      "cenário local.\n",
      "A implantação da infraestrutura e o desenvolvimento dos projetos têm como objetivo\n",
      "avançar gradualmente até níveis mais altos de maturidade. Espera-se que, ao final do\n",
      "programa, as soluções alcançarão conjuntamente TRL 7 ou superior para a implantação da\n",
      "TV 3.0, estágio no qual um sistema ou protótipo é demonstrado em ambiente operacional\n",
      "próximo ao real. Dessa forma, as soluções entregues pelos projetos derivados poderão\n",
      "Página 10 de 37\n",
      "Plano de Utilização de Recursos – PUR nº 495/2025 aplicado aos Projetos Prioritários\n",
      "Ano-Base 2025\n"
     ]
    }
   ],
   "source": [
    "# 2. RAG Engine Setup\n",
    "# Initialize RAG Engine\n",
    "rag = RAGEngine(\n",
    "    embedding_model_name=settings.EMBEDDING_MODEL_NAME,\n",
    "    vector_store_path=settings.VECTOR_DB_PATH\n",
    ")\n",
    "\n",
    "# Create Vector Store (Run this once or when docs change)\n",
    "rag.create_vector_store(documents)\n",
    "\n",
    "# Load it back (to test loading)\n",
    "rag.load_vector_store()\n",
    "\n",
    "# Test Retrieval\n",
    "# Increased k to 10 to ensure enough context is retrieved for the audit\n",
    "retriever = rag.get_retriever(k=10)\n",
    "docs = retriever.invoke(\"Qual o TRL do projeto?\")\n",
    "print(f\"Retrieved {len(docs)} docs.\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to AWS Bedrock (Region: us-east-1)...\n",
      "✅ AWS Bedrock connected successfully.\n",
      "Running audit... (This may take a few seconds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarsobertolini/Library/Python/3.9/lib/python/site-packages/boto3/compat.py:84: PythonDeprecationWarning: Boto3 will no longer support Python 3.9 starting April 29, 2026. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.10 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n",
      "INFO:langchain_aws.llms.bedrock:Using Bedrock Invoke API to generate response\n",
      "INFO:langchain_aws.llms.bedrock:Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Audit Result ===\n",
      "{\n",
      "  \"project_title\": \"TV 3.0 na Amazônia Ocidental\",\n",
      "  \"trl_level\": 0,\n",
      "  \"methodology_summary\": \"Desenvolvimento de metodologias ágeis e de colaboração para projetos de TV 3.0.\",\n",
      "  \"innovation_highlights\": [\n",
      "    \"Inovação na integração vertical dos processos, uso de inteligência aplicada e capacidade de controle avançado da transmissão e dos testes de TV 3.0.\"\n",
      "  ],\n",
      "  \"team_analysis\": \"Equipe\",\n",
      "  \"financial_analysis\": \"Não informado\",\n",
      "  \"risk_assessment\": \"Médio\",\n",
      "  \"compliance_score\": 0,\n",
      "  \"justification\": \"Sem justificativa fornecida\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 3. Audit Execution\n",
    "import boto3\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "# We will try to use AWS Bedrock first (since you configured AWS creds)\n",
    "# If that fails, we fall back to OpenAI if key is present.\n",
    "\n",
    "llm = None\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to connect to AWS Bedrock (Region: {settings.AWS_REGION})...\")\n",
    "    \n",
    "    # Create Bedrock client\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\", \n",
    "        region_name=settings.AWS_REGION\n",
    "    )\n",
    "    \n",
    "    llm = ChatBedrock(\n",
    "        model_id=settings.LLM_MODEL_ID, \n",
    "        client=bedrock_client,\n",
    "        model_kwargs={\"temperature\": 0.0}\n",
    "    )\n",
    "    print(\"✅ AWS Bedrock connected successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not connect to AWS Bedrock: {e}\")\n",
    "    print(\"Checking for OpenAI API Key...\")\n",
    "    \n",
    "    if settings.OPENAI_API_KEY:\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        print(\"✅ Connected to OpenAI.\")\n",
    "    else:\n",
    "        print(\"❌ No valid LLM configuration found. Please check your .env file.\")\n",
    "\n",
    "if llm:\n",
    "    auditor = AuditorAgent(llm=llm, retriever=retriever)\n",
    "    \n",
    "    # Run Audit\n",
    "    print(\"Running audit... (This may take a few seconds)\")\n",
    "    result = auditor.audit_project(\"Analise este projeto e extraia as métricas de conformidade.\")\n",
    "    \n",
    "    print(\"\\n=== Audit Result ===\")\n",
    "    # Pydantic v2 uses model_dump_json() instead of json()\n",
    "    print(result.model_dump_json(indent=2))\n",
    "else:\n",
    "    print(\"Skipping audit due to missing LLM.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
